# -*- coding: utf-8 -*-
"""6-Self-Supervised-Trainer-(Stage2-Colorizer)-MODIS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MV7E4F5PlMVzjHxXYI5RI-YhiBLB4e6p

**GET DATA**

Summary of this notebook: ...

Todo:

1. Self supervised representation learning using colorization as a pretext task
2. Taking in an image I , rotating it by an a set angle A--> rotated image (I_A), rotated angle A (actual label) (Done)
3. Dataloader that does this somehow (Done)
4. Some model-> ResNet50  + new full-connected layer (feature vector-> probability that image belongs to one of n classes) (Done-ish)
5. Additional training with the above model minimizing the loss wrt. actual rotated angle
f(I_A) = min[L(A_predict,A)]
6. Save this new model

7. Proof of concept:
Imagenet-> Train on Caltech validation on caltech




Definition of Done: ...

# Imports
"""

# # Mount Google drive
# from google.colab import drive
# import os
# drive.mount('/content/gdrive')

# Change to current dataset
import os
import sys
sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')
sys.dont_write_bytecode = True

# Imports from Colab 2
import math
import numpy as np
import pickle
import keras
import tensorflow
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from datetime import datetime
# Import pretrained model
from tensorflow.keras.applications import MobileNet, ResNet50
from tensorflow.keras.applications.resnet50 import preprocess_input

# Imports for Colab 6
import cv2  # Read raw image
import glob
# from google.colab.patches import cv2_imshow
from matplotlib import pyplot as plt
from scipy import ndimage  # For rotation task or
import imutils
from tensorflow.keras import Model
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.optimizers import Adam

from tensorflow.python.keras.utils import data_utils
from tensorflow.keras.preprocessing.image import Iterator


# Imports for Colorizer
from os import path
from tensorflow.keras.layers import Conv2D, Conv2DTranspose, UpSampling2D, AveragePooling2D, Reshape, Conv2DTranspose, ZeroPadding2D
from tensorflow.keras.layers import Activation, InputLayer, BatchNormalization
from tensorflow.keras.callbacks import TensorBoard
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb
from skimage.io import imsave
import random
import tensorflow as tf

# Imports for Autoencoder
import utils
from network_definition import encoder, decoder
from utils_data import parser, from_tfrecords
from keras.layers import Input
from keras.models import Model
# Check to see if GPU is being used
tensorflow.test.gpu_device_name()

dims = (400, 400, 3)
bool_generate = False
BATCH_SIZE = 64

root_dir = "/home/satyarth934/Projects/NASA_FDL_2020/Datasets/MODIS_MCD43A4/"
train_tfrecords_dir = root_dir + "tfrecords/train/train.tfrecords-*"
valid_tfrecords_dir = root_dir + "tfrecords/valid/valid.tfrecords-*"
test_tfrecords_dir = root_dir + "tfrecords/test/test.tfrecords-*"

train_tfrecords = glob.glob(train_tfrecords_dir)
valid_tfrecords = glob.glob(valid_tfrecords_dir)
test_tfrecords = glob.glob(test_tfrecords_dir)

"""# Dataloader creation and test"""

train_dataset = from_tfrecords(records_globs=train_tfrecords,
                               split="train",
                               batch_size=BATCH_SIZE)

valid_dataset = from_tfrecords(records_globs=valid_tfrecords,
                               split="valid",
                               batch_size=BATCH_SIZE)

print("LEN TRAINING:",len(train_tfrecords))
"""# Model"""

#model = nd.Autoencoder(is_fusion=False,depth_after_fusion=256)
#print("---------------------------------------------")
#print("Encoder:")
#print(model.encoder.summary())
#print("Encoder out shape:",model.encoder.output_shape)
#print("Decoder:")
#print(model.decoder.summary())
#print("Decoder out shape:",model.decoder.output_shape)

#image=Input(shape=dims)
encoder_model=encoder(dims)
print("Encoder")
print(encoder_model.summary())

encoding_depth=15
decoder_model=decoder(encoder_model.output_shape[1:])
print("Decoder")
print(decoder_model.summary())

complete_model=Sequential()
complete_model.add(encoder_model)
complete_model.add(decoder_model)
complete_model.build(input_shape=(None,*dims))

print("Complete Model")
print(complete_model.summary())


"""# Model Training"""

# model.compile(optimizer='rmsprop', loss='mse', metrics = ['accuracy'])
complete_model.compile(optimizer='rmsprop',loss='mse',metrics=['accuracy'])


callback_earlystop = EarlyStopping(monitor='loss', patience=5)

checkpoint_filepath = root_dir + 'Models/checkpoint/vanilla_ae_{epoch:04d}.h5'
callback_checkpoint = ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=False,
    period=1)


complete_model.fit(train_dataset,
        epochs=100,
        steps_per_epoch=len(train_tfrecords)/BATCH_SIZE,
        callbacks=[callback_earlystop,callback_checkpoint])

now = datetime.now()
dt_string = now.strftime("%d_%m_%H_%M")
print(dt_string)
complete_model.save('Models/vanilla_ae_MODIS_Exp_Custom_' + dt_string)
