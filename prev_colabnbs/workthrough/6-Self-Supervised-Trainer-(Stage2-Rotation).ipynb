{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"6-Self-Supervised-Trainer-(Stage2-Rotation).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"561rrpfX_VaY","colab_type":"text"},"source":["**GET DATA**\n","\n","Summary of this notebook: ...\n","\n","Todo:\n","\n","1. Self supervised representation learning using rotation as a pretext task\n","2. Taking in an image I , rotating it by an a set angle A--> rotated image (I_A), rotated angle A (actual label) (Done)\n","3. Dataloader that does this somehow (Done)\n","4. Some model-> ResNet50  + new full-connected layer (feature vector-> probability that image belongs to one of n classes) (Done-ish)\n","5. Additional training with the above model minimizing the loss wrt. actual rotated angle\n","f(I_A)=min[L(A_predict,A)]\n","6. Save this new model\n","\n","7. Proof of concept:\n","Imagenet-> Train on Caltech validation on caltech\n","\n","\n","\n","\n","Definition of Done: ..."]},{"cell_type":"markdown","metadata":{"id":"6Ev8fWuVejb0","colab_type":"text"},"source":["# Imports\n"]},{"cell_type":"code","metadata":{"id":"z81q60U3G0ol","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1593666388105,"user_tz":420,"elapsed":528,"user":{"displayName":"Nishan Srishankar","photoUrl":"","userId":"11074479106454986489"}},"outputId":"a0dc2d00-6bf0-4faf-d4ae-83b33a9733fb"},"source":["# Mount Google drive\n","from google.colab import drive\n","import os\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YgicpEBiG2d8","colab_type":"code","colab":{}},"source":["# Change to current dataset\n","os.chdir(\"/content/gdrive/Shared drives/2020_FDLUSA_Earth Science_Knowledge Discovery Framework/Code\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uj83O8YKSdVR","colab_type":"code","colab":{}},"source":["# !pip install split-folders\n","# dataset = \"UCMerced_LandUse\"\n","# dataPath = \"Datasets/\"+ dataset+ \"/Images\"\n","# import split_folders\n","# split_folders.ratio(dataPath+'/',output=dataPath+'/Splits_2/', seed=1337, ratio=(.8, .2, .0)) # default values\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TKfFt17nGyDm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593666393760,"user_tz":420,"elapsed":1786,"user":{"displayName":"Nishan Srishankar","photoUrl":"","userId":"11074479106454986489"}},"outputId":"d69c42d5-f67f-442a-e546-b6ab43bdd4aa"},"source":["# Imports from Colab 2\n","import os\n","import math\n","import numpy as np\n","import pickle\n","import keras\n","import tensorflow\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# Import pretrained model\n","from tensorflow.keras.applications import MobileNet, ResNet50\n","from tensorflow.keras.applications.resnet50 import preprocess_input"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"gFkitrY7HMwz","colab_type":"code","colab":{}},"source":["# Imports for Colab 6\n","import cv2 # Read raw image\n","import glob\n","from google.colab.patches import cv2_imshow\n","from matplotlib import pyplot as plt\n","from scipy import ndimage # For rotation task or\n","import imutils\n","from tensorflow.keras import Model\n","from tensorflow.keras.layers import Dense, Flatten, Dropout\n","from tensorflow.keras.optimizers import Adam\n","\n","from tensorflow.python.keras.utils import data_utils\n","from tensorflow.keras.preprocessing.image import Iterator"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7AX-W5mqG4xD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593666398244,"user_tz":420,"elapsed":1423,"user":{"displayName":"Nishan Srishankar","photoUrl":"","userId":"11074479106454986489"}},"outputId":"70777b2d-e5fd-4630-b5ae-f7c0d9239c8f"},"source":["# Check to see if GPU is being used\n","tensorflow.test.gpu_device_name()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic":{"type":"string"},"text/plain":["'/device:GPU:0'"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"KzmeP_R9ep5w","colab_type":"text"},"source":["# Data Augmentation/Analysis"]},{"cell_type":"code","metadata":{"id":"n2rPBilY_w-X","colab_type":"code","colab":{}},"source":["# Raw dataset/path/model paths\n","dataset = \"UCMerced_LandUse\"\n","dataPath = (\"Datasets/\"+ dataset+ \"/Splits_2\")\n","modelName = \"MobileNet\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JUXadS-8JTpB","colab_type":"code","colab":{}},"source":["#  Image modification functions- to plot image as well as to do a rotation task\n","def disp_mpl(im_arr): # Matplotlib imshow\n","  plt.imshow(im_arr)\n","  plt.show()\n","\n","def disp_cv2(im_arr): # OpenCV2 image\n","  cv2_imshow(im_arr)\n","\n","def basic_rotate(im_arr,angle): # Basic Scipy ndimage rotation. Results in a black border\n","  return ndimage.rotate(im_arr,angle,reshape=True)\n","\n","# If we really want to do this: https://stackoverflow.com/questions/16702966/rotate-image-and-crop-out-black-borders\n","def rotate_image(image, angle):\n","    \"\"\"\n","    Rotates an OpenCV 2 / NumPy image about it's centre by the given angle\n","    (in degrees). The returned image will be large enough to hold the entire\n","    new image, with a black background\n","    \"\"\"\n","\n","    # Get the image size\n","    # No that's not an error - NumPy stores image matricies backwards\n","    image_size = (image.shape[1], image.shape[0])\n","    image_center = tuple(np.array(image_size) / 2)\n","\n","    # Convert the OpenCV 3x2 rotation matrix to 3x3\n","    rot_mat = np.vstack(\n","        [cv2.getRotationMatrix2D(image_center, angle, 1.0), [0, 0, 1]]\n","    )\n","\n","    rot_mat_notranslate = np.matrix(rot_mat[0:2, 0:2])\n","\n","    # Shorthand for below calcs\n","    image_w2 = image_size[0] * 0.5\n","    image_h2 = image_size[1] * 0.5\n","\n","    # Obtain the rotated coordinates of the image corners\n","    rotated_coords = [\n","        (np.array([-image_w2,  image_h2]) * rot_mat_notranslate).A[0],\n","        (np.array([ image_w2,  image_h2]) * rot_mat_notranslate).A[0],\n","        (np.array([-image_w2, -image_h2]) * rot_mat_notranslate).A[0],\n","        (np.array([ image_w2, -image_h2]) * rot_mat_notranslate).A[0]\n","    ]\n","\n","    # Find the size of the new image\n","    x_coords = [pt[0] for pt in rotated_coords]\n","    x_pos = [x for x in x_coords if x > 0]\n","    x_neg = [x for x in x_coords if x < 0]\n","\n","    y_coords = [pt[1] for pt in rotated_coords]\n","    y_pos = [y for y in y_coords if y > 0]\n","    y_neg = [y for y in y_coords if y < 0]\n","\n","    right_bound = max(x_pos)\n","    left_bound = min(x_neg)\n","    top_bound = max(y_pos)\n","    bot_bound = min(y_neg)\n","\n","    new_w = int(abs(right_bound - left_bound))\n","    new_h = int(abs(top_bound - bot_bound))\n","\n","    # We require a translation matrix to keep the image centred\n","    trans_mat = np.matrix([\n","        [1, 0, int(new_w * 0.5 - image_w2)],\n","        [0, 1, int(new_h * 0.5 - image_h2)],\n","        [0, 0, 1]\n","    ])\n","\n","    # Compute the tranform for the combined rotation and translation\n","    affine_mat = (np.matrix(trans_mat) * np.matrix(rot_mat))[0:2, :]\n","\n","    # Apply the transform\n","    result = cv2.warpAffine(\n","        image,\n","        affine_mat,\n","        (new_w, new_h),\n","        flags=cv2.INTER_LINEAR\n","    )\n","\n","    return result\n","\n","\n","def largest_rotated_rect(w, h, angle):\n","    \"\"\"\n","    Given a rectangle of size wxh that has been rotated by 'angle' (in\n","    radians), computes the width and height of the largest possible\n","    axis-aligned rectangle within the rotated rectangle.\n","\n","    Original JS code by 'Andri' and Magnus Hoff from Stack Overflow\n","\n","    Converted to Python by Aaron Snoswell\n","    \"\"\"\n","\n","    quadrant = int(math.floor(angle / (math.pi / 2))) & 3\n","    sign_alpha = angle if ((quadrant & 1) == 0) else math.pi - angle\n","    alpha = (sign_alpha % math.pi + math.pi) % math.pi\n","\n","    bb_w = w * math.cos(alpha) + h * math.sin(alpha)\n","    bb_h = w * math.sin(alpha) + h * math.cos(alpha)\n","\n","    gamma = math.atan2(bb_w, bb_w) if (w < h) else math.atan2(bb_w, bb_w)\n","\n","    delta = math.pi - alpha - gamma\n","\n","    length = h if (w < h) else w\n","\n","    d = length * math.cos(alpha)\n","    a = d * math.sin(alpha) / math.sin(delta)\n","\n","    y = a * math.cos(gamma)\n","    x = y * math.tan(gamma)\n","\n","    return (\n","        bb_w - 2 * x,\n","        bb_h - 2 * y\n","    )\n","\n","\n","def crop_around_center(image, width, height):\n","    \"\"\"\n","    Given a NumPy / OpenCV 2 image, crops it to the given width and height,\n","    around it's centre point\n","    \"\"\"\n","\n","    image_size = (image.shape[1], image.shape[0])\n","    image_center = (int(image_size[0] * 0.5), int(image_size[1] * 0.5))\n","\n","    if(width > image_size[0]):\n","        width = image_size[0]\n","\n","    if(height > image_size[1]):\n","        height = image_size[1]\n","\n","    x1 = int(image_center[0] - width * 0.5)\n","    x2 = int(image_center[0] + width * 0.5)\n","    y1 = int(image_center[1] - height * 0.5)\n","    y2 = int(image_center[1] + height * 0.5)\n","\n","    return image[y1:y2, x1:x2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NWy-80_PHlfe","colab_type":"code","colab":{}},"source":["# Test rotation\n","# angle=45\n","# sample_image_path=dataPath+'/river/river00.tif'\n","# image=cv2.imread(sample_image_path)\n","# h,w,_=image.shape\n","# rotated_image=rotate_image(image,angle)\n","\n","# image_rotated_cropped = crop_around_center(rotated_image,*largest_rotated_rect(w,h,math.radians(angle)))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M7Tqo3-1WHWk","colab_type":"code","colab":{}},"source":["# disp_cv2(image)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yFNdRQCoPDMj","colab_type":"code","colab":{}},"source":["# image.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v7AEVo4kZ2vz","colab_type":"code","colab":{}},"source":["# disp_cv2(rotated_image)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MoX9Sc6ZI_si","colab_type":"code","colab":{}},"source":["# disp_cv2(image_rotated_cropped)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eXQSwYELNlKd","colab_type":"code","colab":{}},"source":["# image_rotated_cropped.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FNBiKT_Qe0xr","colab_type":"text"},"source":["# Dataloader creation and test"]},{"cell_type":"code","metadata":{"id":"u5i8MzfpIoAM","colab_type":"code","colab":{}},"source":["# Glob through all the images we have in the current directory\n","# image_paths=glob.glob(dataPath+'/**/*.tif')\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cnwO8mvVvWFT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1593666419710,"user_tz":420,"elapsed":814,"user":{"displayName":"Nishan Srishankar","photoUrl":"","userId":"11074479106454986489"}},"outputId":"906ec289-0700-4613-cd4b-a7d4516e9f75"},"source":["train_dataGenerator = ImageDataGenerator(preprocessing_function=preprocess_input,\n","                                   width_shift_range=0.1,\n","                                   height_shift_range=0.1,\n","                                   zoom_range=0.1,\n","                                   channel_shift_range=0.1)\n","\n","valid_dataGenerator=ImageDataGenerator(preprocessing_function=preprocess_input)\n","\n","\n","batch_size = 32\n","trainGenerator = train_dataGenerator.flow_from_directory(\n","        dataPath+'/train/',\n","        target_size=(224, 224),\n","        batch_size= batch_size,\n","        class_mode= None, \n","        shuffle = False)\n","\n","testGenerator=valid_dataGenerator.flow_from_directory(\n","        dataPath+'/val/',\n","        target_size=(224, 224),\n","        batch_size= batch_size,\n","        class_mode= None, \n","        shuffle = False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 1680 images belonging to 21 classes.\n","Found 420 images belonging to 21 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ri-SDcocunWM","colab_type":"code","colab":{}},"source":["# X here is the yield output from a data generator (test)\n","# def batch_augment(flow_gen,n_classes):\n","#   print(flow_gen.shape)\n","#   for batch_images in flow_gen:\n","#     X=np.empty_like(flow_gen.shape)\n","#     y=np.empty((len(flow_gen),n_classes))\n","\n","#     for im in batch_images:\n","#       print(im.shape)\n","#       h,w,_=im.shape\n","#       angle_categorical=np.random.randint(low=0,high=n_classes)\n","#       angle=angle_categorical*90\n","#       rotated_image=rotate_image(im,angle)\n","      \n","#       image_rotated_cropped = crop_around_center(rotated_image,*largest_rotated_rect(w,h,math.radians(angle)))\n","#       h,w,_=image_rotated_cropped.shape\n","      \n","#       center=(h//2,w//2)\n","#       d_h=246\n","#       X[i,]= image_rotated_cropped[int(center[0]-d_h/2):int(center[0]+d_h/2), int(center[1]-d_h/2):int(center[1]+d_h/2)]\n","#       y[i]=angle_categorical\n","#     yield X,keras.utils.to_categorical(y, num_classes=n_classes)\n","\n","    # X here is the yield output from a data generator (test)\n","def batch_augment(flow_gen,dim,n_classes=4):\n","  \n","  while True:\n","    X=np.empty((0,*dim))\n","    y=np.empty((0,1))\n","    for i,batch_im in enumerate(flow_gen[0]):\n","      h,w,_=batch_im.shape\n","      angle_categorical=np.random.randint(low=0,high=n_classes)\n","      angle=angle_categorical*90\n","      rotated_image=rotate_image(batch_im,angle)\n","        \n","      image_rotated_cropped = crop_around_center(rotated_image,*largest_rotated_rect(w,h,math.radians(angle)))\n","      h_c,w_c,_=image_rotated_cropped.shape\n","      # print(h_c,w_c)\n","      center=(h_c//2,w_c//2)\n","      # print(center)\n","      \n","      d_h=200\n","      # print(int(center[0]-d_h/2),int(center[0]+d_h/2))\n","      # print(int(center[1]-d_h/2),int(center[1]+d_h/2))\n","      aug_batch_im=image_rotated_cropped[int(center[0]-d_h/2):int(center[0]+d_h/2), int(center[1]-d_h/2):int(center[1]+d_h/2)]\n","      aug_batch_label=angle_categorical\n","      # print(X.shape,np.array(aug_batch_im).shape)\n","      # print(y.shape,np.array([aug_batch_label]).shape)\n","      X=np.append(X,np.expand_dims(aug_batch_im, axis=0),axis=0)\n","      y=np.append(y,np.expand_dims(np.array([aug_batch_label]), axis=0),axis=0)\n","    yield X,keras.utils.to_categorical(y, num_classes=n_classes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O_tgYfwv1pQ3","colab_type":"code","colab":{}},"source":["# augmented_generator=batch_augment(trainGenerator,(200,200,3),4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0FRw5RSccnuq","colab_type":"code","colab":{}},"source":["# augmented_testgenerator=batch_augment(testGenerator,(200,200,3),4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4cUDTMBbQ768","colab_type":"code","colab":{}},"source":["# Link: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n","\n","# Create a custom dataloader that performs a rotation task as well as generates a pseudo-label (in __data_generation)\n","# Put in the output image size and number of classes as a variable\n","\n","\n","# class RandomFramesFromPathsToVideos(data_utils.Sequence):\n","'''\n","class DataGenerator(data_utils.Sequence):\n","    'Generates data for Keras'\n","    def __init__(self, list_IDs, batch_size=32, dim=(246,246,3),\n","                 n_classes=4, shuffle=True):\n","        'Initialization'\n","        self.dim = dim\n","        self.batch_size = batch_size\n","        self.list_IDs = list_IDs\n","        self.n_classes = n_classes\n","        self.shuffle = shuffle\n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        'Denotes the number of batches per epoch'\n","        return int(np.floor(len(self.list_IDs) / self.batch_size))\n","\n","    def __getitem__(self, index):\n","        'Generate one batch of data'\n","        # Generate indexes of the batch\n","        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n","\n","        # Find list of IDs\n","        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n","\n","        # Generate data\n","        X, y = self.__data_generation(list_IDs_temp)\n","\n","        return X, y\n","\n","    def on_epoch_end(self):\n","        'Updates indexes after each epoch'\n","        self.indexes = np.arange(len(self.list_IDs))\n","        if self.shuffle == True:\n","            np.random.shuffle(self.indexes)\n","\n","    def __data_generation(self, list_IDs_temp):\n","        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n","        # Initialization\n","        X = np.empty((self.batch_size, *self.dim))\n","        y = np.empty((self.batch_size), dtype=int)\n","        for i in range(len(list_IDs_temp)): \n","          temp_image=cv2.imread(list_IDs_temp[i])\n","          h,w,_=temp_image.shape\n","          angle_categorical=np.random.randint(low=0,high=4) # 0,1,2,3\n","          angle=angle_categorical*90\n","          rotated_image=rotate_image(temp_image,angle)\n","\n","          image_rotated_cropped = crop_around_center(\n","            rotated_image,\n","            *largest_rotated_rect(\n","                w,\n","                h,\n","                math.radians(angle)\n","            )\n","          )\n","          h,w,_=image_rotated_cropped.shape\n","          center=(h//2,w//2)\n","          d_h=246\n","          X[i,]= image_rotated_cropped[int(center[0]-d_h/2):int(center[0]+d_h/2), int(center[1]-d_h/2):int(center[1]+d_h/2)]\n","          y[i]=angle_categorical\n","\n","        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)\n","\n","'''\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vq9nGokYLZEd","colab_type":"code","colab":{}},"source":["# Instantiate train generator\n","# train_generator=DataGenerator(list_IDs=image_paths)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7JOWs0MfPikM","colab_type":"code","colab":{}},"source":["# Grab batch at index 0\n","# x,y=train_generator.__getitem__(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TAeWrAhzPgFm","colab_type":"code","colab":{}},"source":["# Test shapes\n","# print(x.shape)\n","# print(y.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yODvuUiBSwMg","colab_type":"code","colab":{}},"source":["# Validate the data augmentation\n","# disp_cv2(x[2])\n","# print(y[2])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1QAgBBBNfDAx","colab_type":"text"},"source":["# Model creation"]},{"cell_type":"code","metadata":{"id":"6S5QVLNOUDLV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593666432587,"user_tz":420,"elapsed":3416,"user":{"displayName":"Nishan Srishankar","photoUrl":"","userId":"11074479106454986489"}},"outputId":"3c324e1a-689f-407f-a58a-4cd130825e76"},"source":["# Base model is a Mobile net (from Colab 2)\n","base_model = ResNet50(\n","    input_shape = (200, 200, 3),\n","    include_top = False,\n","    weights = 'imagenet',\n","    pooling = \"max\"\n",")\n","print(base_model.output.shape)\n","\n","# Head model consists of a 1 Dense layer (could be improved later) to output for the number of classes\n","head_model=Flatten()(base_model.output)\n","head_model=Dense(512,activation='relu')(head_model)\n","head_model=Dropout(0.3)(head_model)\n","head_model=Dense(128,activation='relu')(head_model)\n","head_model=Dropout(0.2)(head_model)\n","predictions=Dense(units=8,activation='softmax')(head_model)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(None, 2048)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cTX0tlz9m-5Q","colab_type":"code","colab":{}},"source":["# Base model is a Mobile net (from Colab 2)\n","# base_model_untr = MobileNet(\n","#     input_shape = (200, 200, 3),\n","#     include_top = False,\n","#     weights = None,\n","#     pooling = \"max\"\n","# )\n","# print(base_model_untr.output.shape)\n","\n","# # Head model consists of a 1 Dense layer (could be improved later) to output for the number of classes\n","# head_model_untr=Flatten()(base_model_untr.output)\n","# head_model_untr=Dense(256,activation='relu')(head_model_untr)\n","# head_model_untr=Dropout(0.2)(head_model_untr)\n","# predictions_untr=Dense(units=4,activation='softmax')(head_model_untr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gQS2pQtXnDmV","colab_type":"code","colab":{}},"source":["# complete_model_untr=Model(base_model_untr.input,predictions_untr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RG3bnd_0UQqN","colab_type":"code","colab":{}},"source":["# Stitch the models together (not compiled yet)\n","complete_model=Model(base_model.input,predictions)\n","# complete_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tsgBqD4dfHoE","colab_type":"text"},"source":["# Model Training"]},{"cell_type":"markdown","metadata":{"id":"E3SPgBvIHkAQ","colab_type":"text"},"source":["complete_model=[base_model,head_model]\n","Base Model--> Convolution layers trained on Imagenet\n","Head Model--> Dense layer(s) w/ no training\n","\n","1. Train dense layer to be good\n","2. Train part of the base model to be a bit better."]},{"cell_type":"code","metadata":{"id":"iQmU9qmBHxWU","colab_type":"code","colab":{}},"source":["opt=Adam(learning_rate=1e-3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8NKqAjGiIhu8","colab_type":"text"},"source":["## Training Cycles\n","#### First: Freeze the base model and just train the dense models at the head"]},{"cell_type":"code","metadata":{"id":"LoPPvZ9Gkm7_","colab_type":"code","colab":{}},"source":["for l in base_model.layers:\n","  l.trainable=False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kV6JnF1pKDGO","colab_type":"code","colab":{}},"source":["complete_model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YdOe_DrN625Q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":447},"executionInfo":{"status":"error","timestamp":1593666620363,"user_tz":420,"elapsed":1000,"user":{"displayName":"Nishan Srishankar","photoUrl":"","userId":"11074479106454986489"}},"outputId":"72596c8a-33e7-4f3a-82c1-9704302429a1"},"source":["complete_model.fit(batch_augment(trainGenerator,(200,200,3),4),\n","                   validation_data=batch_augment(testGenerator,(200,200,3),4),\n","                   steps_per_epoch=1680/32,\n","                   validation_steps=420/32,\n","                   epochs=5)\n"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-3a4a7782d523>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                    \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1680\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                    \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m420\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                    epochs=5)\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    813\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m           model=self)\n\u001b[0m\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[1;32m    770\u001b[0m     \u001b[0;31m# Since we have to know the dtype of the python generator when we build the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0;31m# dataset, we have to look at a batch to infer the structure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m     \u001b[0mpeek\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_peek_and_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m     \u001b[0massert_not_namedtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_peek_and_restore\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    828\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_peek_and_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m     \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpeek\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-1b9324c9dcab>\u001b[0m in \u001b[0;36mbatch_augment\u001b[0;34m(flow_gen, dim, n_classes)\u001b[0m\n\u001b[1;32m     47\u001b[0m       \u001b[0;31m# print(X.shape,np.array(aug_batch_im).shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m       \u001b[0;31m# print(y.shape,np.array([aug_batch_label]).shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m       \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrotated_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maug_batch_label\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mappend\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   4691\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4692\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4693\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 316 and the array at index 1 has size 224"]}]},{"cell_type":"markdown","metadata":{"id":"2Uh0-201IuVR","colab_type":"text"},"source":["#### Second: Unfreeze the last \"part\" of the base model and just train"]},{"cell_type":"code","metadata":{"id":"GqMWXjgDI1cx","colab_type":"code","colab":{}},"source":["trainGenerator.reset()\n","testGenerator.reset()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xWZf2qn3hQsF","colab_type":"code","colab":{}},"source":["start_from=int(len(base_model.layers)*(1/2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PJYdlfkmhqZy","colab_type":"code","colab":{}},"source":["for layer in base_model.layers[start_from:]:\n","  layer.trainable=True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QEG6Amv-h0rz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":412},"executionInfo":{"status":"ok","timestamp":1593665549373,"user_tz":420,"elapsed":324684,"user":{"displayName":"Nishan Srishankar","photoUrl":"","userId":"11074479106454986489"}},"outputId":"dd6d575e-a1d8-43d5-8202-16e92acf5785"},"source":["complete_model.fit(batch_augment(trainGenerator,(200,200,3),4),\n","                   validation_data=batch_augment(testGenerator,(200,200,3),4),\n","                   steps_per_epoch=1680/32,\n","                   validation_steps=420/32,\n","                   epochs=10)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","53/52 [==============================] - 32s 607ms/step - loss: 0.8695 - accuracy: 0.5419 - val_loss: 2.0356 - val_accuracy: 0.3147\n","Epoch 2/10\n","53/52 [==============================] - 32s 610ms/step - loss: 0.8161 - accuracy: 0.5772 - val_loss: 1.7743 - val_accuracy: 0.3750\n","Epoch 3/10\n","53/52 [==============================] - 32s 608ms/step - loss: 0.7829 - accuracy: 0.5820 - val_loss: 2.3329 - val_accuracy: 0.2790\n","Epoch 4/10\n","53/52 [==============================] - 33s 616ms/step - loss: 0.7112 - accuracy: 0.6291 - val_loss: 2.1676 - val_accuracy: 0.3214\n","Epoch 5/10\n","53/52 [==============================] - 32s 605ms/step - loss: 0.7230 - accuracy: 0.6073 - val_loss: 1.6936 - val_accuracy: 0.3304\n","Epoch 6/10\n","53/52 [==============================] - 32s 611ms/step - loss: 0.6953 - accuracy: 0.6327 - val_loss: 1.9338 - val_accuracy: 0.2991\n","Epoch 7/10\n","53/52 [==============================] - 32s 602ms/step - loss: 0.6413 - accuracy: 0.6781 - val_loss: 2.1702 - val_accuracy: 0.3170\n","Epoch 8/10\n","53/52 [==============================] - 32s 607ms/step - loss: 0.6198 - accuracy: 0.6834 - val_loss: 3.0937 - val_accuracy: 0.3281\n","Epoch 9/10\n","53/52 [==============================] - 32s 604ms/step - loss: 0.5775 - accuracy: 0.6904 - val_loss: 2.3208 - val_accuracy: 0.2656\n","Epoch 10/10\n","53/52 [==============================] - 32s 604ms/step - loss: 0.5437 - accuracy: 0.7288 - val_loss: 2.1220 - val_accuracy: 0.3170\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f8633ea4278>"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"kKAAVsxOilTL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":108},"executionInfo":{"status":"ok","timestamp":1593665621835,"user_tz":420,"elapsed":18947,"user":{"displayName":"Nishan Srishankar","photoUrl":"","userId":"11074479106454986489"}},"outputId":"e4aa4c83-0d76-4417-b5a1-13e4dad5e9e1"},"source":["complete_model.save('Models/Rotation_Resnet50')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","INFO:tensorflow:Assets written to: Models/Rotation_Resnet50/assets\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"L9uP6nqmn6Ay","colab_type":"text"},"source":["## Test on untrained MobileNet+Dense layer\n"]},{"cell_type":"code","metadata":{"id":"QHEQ0cvunRBz","colab_type":"code","colab":{}},"source":["# for l in base_model_untr.layers:\n","#   l.trainable=False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nvlYIa3xnTvx","colab_type":"code","colab":{}},"source":["# complete_model_untr.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MfDSGSRCnT47","colab_type":"code","colab":{}},"source":["# complete_model_untr.fit(batch_augment(trainGenerator,(200,200,3),4),steps_per_epoch=2100/32,epochs=5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wW_amXYznhzx","colab_type":"code","colab":{}},"source":["# trainGenerator.reset()\n","# for layer in base_model_untr.layers[start_from:]:\n","#   layer.trainable=True\n","# complete_model_untr.fit(batch_augment(trainGenerator,(200,200,3),4),steps_per_epoch=2100/32,epochs=5)\n"],"execution_count":null,"outputs":[]}]}