{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "211e6c7d-d258-4e31-8216-ac33886a0f23"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Data-Augmentation/Analysis\" data-toc-modified-id=\"Data-Augmentation/Analysis-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Augmentation/Analysis</a></span></li><li><span><a href=\"#Dataloader-creation-and-test\" data-toc-modified-id=\"Dataloader-creation-and-test-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Dataloader creation and test</a></span></li><li><span><a href=\"#Model-creation\" data-toc-modified-id=\"Model-creation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Model creation</a></span></li><li><span><a href=\"#Model-Training\" data-toc-modified-id=\"Model-Training-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Model Training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-Testing\" data-toc-modified-id=\"Model-Testing-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Model Testing</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "561rrpfX_VaY",
    "nbpresent": {
     "id": "8902066f-f256-4e6f-8533-16bcb4553d9a"
    }
   },
   "source": [
    "**GET DATA**\n",
    "\n",
    "Summary of this notebook: ...\n",
    "\n",
    "Definition of Done: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Ev8fWuVejb0",
    "nbpresent": {
     "id": "72814092-eab4-4440-910f-bb7b555411ee"
    }
   },
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "TKfFt17nGyDm",
    "nbpresent": {
     "id": "da6aef7a-c579-4384-9373-2cb20ffe43c8"
    },
    "outputId": "5f14dba2-bda1-40e5-c791-d45c75971997"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports from Colab 2\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "import keras\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Import model architecture\n",
    "# from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gFkitrY7HMwz",
    "nbpresent": {
     "id": "91c1d9b8-de8b-4604-b505-21203f08b025"
    }
   },
   "outputs": [],
   "source": [
    "# Imports for Colab 6\n",
    "import sys\n",
    "#sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')\n",
    "\n",
    "import cv2 # Read raw image\n",
    "import glob\n",
    "# from google.colab.patches import cv2_imshow\n",
    "from matplotlib import pyplot as plt\n",
    "# from scipy import ndimage # For rotation task or\n",
    "# import imutils\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.python.keras.utils import data_utils\n",
    "from tensorflow.keras.preprocessing.image import Iterator\n",
    "\n",
    "\n",
    "# Imports for Colorizer\n",
    "from os import path\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, UpSampling2D, AveragePooling2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Reshape, ZeroPadding2D, Add\n",
    "from tensorflow.keras.layers import Activation, InputLayer, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "# from skimage.color import rgb2lab, lab2rgb, rgb2gray\n",
    "# from skimage.io import imsave\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from skimage.transform import resize\n",
    "from tensorflow.keras.layers import PReLU\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import time\n",
    "\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "7AX-W5mqG4xD",
    "nbpresent": {
     "id": "c4acbe4b-3add-4daa-a7e2-ab0b22582c97"
    },
    "outputId": "6178e3ed-be68-483e-b795-2690461f02c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see if GPU is being used\n",
    "tensorflow.test.gpu_device_name()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FNBiKT_Qe0xr",
    "nbpresent": {
     "id": "aab28143-4b01-421e-8433-93df5d6a360b"
    }
   },
   "source": [
    "# Dataloader creation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "id": "cnwO8mvVvWFT",
    "nbpresent": {
     "id": "2e7ba06d-0eb6-476c-9efd-54a61f6f6562"
    },
    "outputId": "bdb606ae-9024-4747-c11b-ef1263a3a03a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train exists\n",
      "Test exists\n"
     ]
    }
   ],
   "source": [
    "# dims=(448,448,3)\n",
    "\n",
    "# train_name='../ssd/proxy_data/merced_xtrain.npy'\n",
    "# test_name='../ssd/proxy_data/merced_xtest.npy'\n",
    "\n",
    "# if path.exists(train_name):\n",
    "#   print(\"Train exists\")\n",
    "#   X_train=np.load(train_name)\n",
    "# else:\n",
    "#   X_train=np.empty((0,*dims))\n",
    "\n",
    "# if path.exists(test_name):\n",
    "#   print(\"Test exists\")\n",
    "#   X_test=np.load(test_name)\n",
    "# else:\n",
    "#   X_test=np.empty((0,*dims))\n",
    "\n",
    "# X_train_reshaped=np.zeros((len(X_train),448,448,3))\n",
    "# X_test_reshaped=np.zeros((len(X_test),448,448,3))\n",
    "\n",
    "# for i in range(len(X_train)):\n",
    "#     X_train_reshaped[i,:]=resize(X_train[i],dims)\n",
    "# print(\"Done\")\n",
    "# del X_train\n",
    "# for i in range(len(X_test)):\n",
    "#     X_test_reshaped[i,:]=resize(X_test[i],dims)\n",
    "# print(\"Done\")\n",
    "# del X_test\n",
    "\n",
    "# np.save('../ssd/proxy_data/merced_xtrain_448.npy',X_train_reshaped)\n",
    "# np.save('../ssd/proxy_data/merced_xtest_448.npy',X_test_reshaped)\n",
    "\n",
    "\n",
    "dims=(448,448,3)\n",
    "\n",
    "train_name='../ssd/proxy_data/merced_xtrain_448.npy'\n",
    "test_name='../ssd/proxy_data/merced_xtest_448.npy'\n",
    "\n",
    "if path.exists(train_name):\n",
    "  print(\"Train exists\")\n",
    "  X_train_reshaped=np.load(train_name)\n",
    "else:\n",
    "  X_train=np.empty((0,*dims))\n",
    "\n",
    "if path.exists(test_name):\n",
    "  print(\"Test exists\")\n",
    "  X_test_reshaped=np.load(test_name)\n",
    "else:\n",
    "  X_test=np.empty((0,*dims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kt6oH37M-_VN",
    "nbpresent": {
     "id": "e858fa5f-c31d-460b-bcee-387900e7bc04"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "AUTOTUNE=tensorflow.data.experimental.AUTOTUNE\n",
    "\n",
    "def convert(image, label):\n",
    "    image = tensorflow.image.convert_image_dtype(image, tf.float32) # Cast and normalize the image to [0,1]\n",
    "    label = tensorflow.image.convert_image_dtype(label, tf.float32)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "# def parse_data(feat,label):\n",
    "#     feat=Resizing(448,448,interpolation='bicubic')\n",
    "#     label=Resizing(448,448,interpolation='bicubic')\n",
    "#     return feat,label\n",
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_reshaped, X_train_reshaped))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_reshaped, X_test_reshaped))\n",
    "\n",
    "train_dataset = train_dataset.map(convert, num_parallel_calls=AUTOTUNE)\n",
    "train_dataset = train_dataset.cache().shuffle(buffer_size=12*batch_size)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "# train_dataset = train_dataset.repeat()\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "test_dataset = test_dataset.map(convert, num_parallel_calls=AUTOTUNE)\n",
    "test_dataset = test_dataset.cache()\n",
    "test_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EklPj1Gg5ZC-",
    "nbpresent": {
     "id": "3a17fa9a-9454-4d40-961c-23b28bffe294"
    }
   },
   "outputs": [],
   "source": [
    "def batch_lab(batch_size,data_generator,data): # Does basically nothing, but just to help with later tasks\n",
    "  for batch in data_generator.flow(data,batch_size=batch_size):\n",
    "    batch=resize(batch,(batch_size,*dims))\n",
    "#     print(np.max(batch),np.min(batch))\n",
    "    yield batch,batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1QAgBBBNfDAx",
    "nbpresent": {
     "id": "995cdb5f-d3bf-4238-a738-2002b1624e98"
    }
   },
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Y1io4UxIu0Vr",
    "nbpresent": {
     "id": "3d354fe2-5913-4420-aa80-d571a244f2a0"
    },
    "outputId": "d22442f8-a7b1-4e40-ea45-67016d9b3500"
   },
   "outputs": [],
   "source": [
    "def encoder(input_shape):\n",
    "\n",
    "    model = Sequential(name=\"encoder\")\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(Conv2D(32, (3, 3), padding=\"same\", strides=2))\n",
    "    model.add(PReLU())\n",
    "    model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "    model.add(PReLU())\n",
    "    model.add(Conv2D(64, (3, 3), padding=\"same\", strides=2))\n",
    "    model.add(PReLU())\n",
    "    model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "    model.add(PReLU())\n",
    "    model.add(Conv2D(128, (3, 3), padding=\"same\", strides=2))\n",
    "    model.add(PReLU())\n",
    "    model.add(Conv2D(256, (3, 3), padding=\"same\"))\n",
    "    model.add(PReLU())\n",
    "    model.add(Conv2D(256, (3, 3), padding=\"same\", strides=2))\n",
    "    model.add(PReLU())\n",
    "    model.add(Conv2D(15, (3, 3), padding=\"same\"))\n",
    "    model.add(PReLU())\n",
    "    model.add(Conv2D(5, (3, 3), padding=\"same\"))\n",
    "    model.add(PReLU())\n",
    "    return model\n",
    "\n",
    "def decoder(input_shape):\n",
    "    model = Sequential(name=\"decoder\")\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(Conv2DTranspose(128, (3, 3), padding=\"same\"))\n",
    "    model.add(PReLU())\n",
    "    model.add(UpSampling2D((2, 2)))\n",
    "    model.add(Conv2DTranspose(64, (3, 3),padding=\"same\"))\n",
    "    model.add(PReLU())\n",
    "    model.add(Conv2DTranspose(64, (3, 3), padding=\"same\"))\n",
    "    model.add(PReLU())\n",
    "    model.add(UpSampling2D((2, 2)))\n",
    "    model.add(Conv2DTranspose(32, (3, 3), padding=\"same\"))\n",
    "    model.add(PReLU())\n",
    "    model.add(Conv2D(3, (3, 3), padding=\"same\"))\n",
    "    model.add(PReLU())\n",
    "    model.add(UpSampling2D((2, 2)))\n",
    "    model.add(Conv2DTranspose(3, (3, 3), padding=\"same\"))\n",
    "    model.add(PReLU())\n",
    "    model.add(UpSampling2D((2, 2)))\n",
    "    model.add(Conv2DTranspose(3, (3, 3), activation=\"tanh\", padding=\"same\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model=encoder(dims)\n",
    "encoding_depth=15\n",
    "\n",
    "decoder_model=decoder(encoder_model.output_shape[1:])\n",
    "\n",
    "complete_model=Sequential()\n",
    "complete_model.add(encoder_model)\n",
    "complete_model.add(decoder_model)\n",
    "\n",
    "complete_model.build(input_shape=(None,*dims))\n",
    "print(complete_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tsgBqD4dfHoE",
    "nbpresent": {
     "id": "e17df5e1-bee5-4d47-a3da-9eb84d49c2c8"
    }
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "LoPPvZ9Gkm7_",
    "nbpresent": {
     "id": "cb64827a-1712-4fec-8ede-ceb510976282"
    },
    "outputId": "268f6216-575f-4bb5-d92e-fbee17baac21"
   },
   "outputs": [],
   "source": [
    "complete_model.compile(optimizer='rmsprop', loss='mse')\n",
    "complete_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "YdOe_DrN625Q",
    "nbpresent": {
     "id": "b0d9a082-fc8c-4e83-9a85-22a6edfcca3d"
    },
    "outputId": "82a08312-5318-4137-8df2-29cf18f9b461"
   },
   "outputs": [],
   "source": [
    "complete_model.fit(train_dataset,\n",
    "                    epochs=100,\n",
    "                    steps_per_epoch=len(X_train_reshaped)/batch_size,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=len(X_test_reshaped)/batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kKAAVsxOilTL",
    "nbpresent": {
     "id": "aaf72caf-bfaa-41ee-93da-e78ce5478950"
    }
   },
   "outputs": [],
   "source": [
    "complete_model.save('../ssd/proxy_models/ae_epoch100_ucmerced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AoNBrpVzOxWJ",
    "nbpresent": {
     "id": "5664feb5-4dfe-4310-b990-708a8287b49b"
    }
   },
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "95ws7iOdej6L",
    "nbpresent": {
     "id": "32975b8d-3448-41db-bc5d-03c4985ec12f"
    }
   },
   "outputs": [],
   "source": [
    "model=load_model('../ssd/proxy_models/ae_epoch100_ucmerced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0xLbk6TMXyW8",
    "nbpresent": {
     "id": "dc72d990-239e-49ee-a077-b32a9a6571c8"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(10):\n",
    "    index=np.random.randint(0,len(X_test_reshaped))\n",
    "\n",
    "    X_test_im=np.expand_dims(X_test_reshaped[index],0)\n",
    "    out_image=np.squeeze(model.predict(X_test_im))\n",
    "    \n",
    "    im_min=out_image.min(axis=(0, 1), keepdims=True)\n",
    "    im_max=out_image.max(axis=(0, 1), keepdims=True)\n",
    "    out_image=(out_image-im_min)/(im_max-im_min)\n",
    "    \n",
    "    \n",
    "    print(\"Orig \",np.min(X_test_im),np.max(X_test_im))\n",
    "    print(\"Gen \",np.min(out_image),np.max(out_image))\n",
    "    fig=plt.figure()\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(X_test_reshaped[index])\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(np.squeeze(X_test_im))\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(out_image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model=encoder(dims)\n",
    "encoding_depth=15\n",
    "\n",
    "decoder_model=decoder(encoder_model.output_shape[1:])\n",
    "\n",
    "model=Sequential()\n",
    "model.add(encoder_model)\n",
    "model.add(decoder_model)\n",
    "\n",
    "model.build(input_shape=(None,*dims))\n",
    "\n",
    "# model.compile(optimizer='rmsprop', loss=custom_loss)\n",
    "# model.summary()\n",
    "opt=RMSprop(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true,y_pred): # Issue-> Non differentiable function\n",
    "    # y_true,y_pred of the shape (bs,h,w,n_c)\n",
    "    loss=tf.Variable(0.,tf.float32)\n",
    "    for i in range(3):\n",
    "        y_true_ch=tf.reshape(y_true[:,:,:,i],[-1])\n",
    "        y_pred_ch=tf.reshape(y_pred[:,:,:,i],[-1])\n",
    "        y_true_hist=tf.histogram_fixed_width(y_true_ch, value_range=(0., 1.), nbins=10) \n",
    "        y_pred_hist=tf.histogram_fixed_width(y_pred_ch, value_range=(0., 1.), nbins=10) \n",
    "        y_true_hist=tf.cast(y_true_hist,dtype=tf.dtypes.float32)\n",
    "        y_pred_hist=tf.cast(y_pred_hist,dtype=tf.dtypes.float32)\n",
    "        ch_loss=K.mean(K.square(y_true_hist-y_pred_hist))\n",
    "#         print(loss)\n",
    "        loss=tf.math.add(loss,ch_loss)\n",
    "#     print(loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def bce_loss(y_true,y_pred):\n",
    "    loss=tf.Variable(0.,tf.float32)\n",
    "    for i in range(3):\n",
    "        bce = tf.keras.losses.BinaryCrossentropy(from_logits=False, label_smoothing=0.25)\n",
    "        y_true_ch=tf.reshape(y_true[:,:,:,i],[-1])\n",
    "        y_pred_ch=tf.reshape(y_pred[:,:,:,i],[-1])\n",
    "        loss_ch=bce(y_true_ch,y_pred_ch)\n",
    "        loss=tf.math.add(loss,loss_ch)\n",
    "    return loss\n",
    "        \n",
    "        \n",
    "# @tf.function\n",
    "def step(model,X,Y):\n",
    "#     print(model.trainable_variables)\n",
    "    with tf.GradientTape(watch_accessed_variables=True) as tape:\n",
    "#         \n",
    "        pred=model(X,training=True)\n",
    "#         loss=custom_loss(Y,pred)\n",
    "        loss=bce_loss(Y,pred)\n",
    "#         tape.watch(model.trainable_variables)\n",
    "    grads=tape.gradient(loss,model.trainable_variables)\n",
    "    return loss,grads\n",
    "\n",
    "# @tf.function\n",
    "def validate(X,Y):\n",
    "    pred=model(X,training=False)\n",
    "    valid_loss=bce_loss(Y,pred)\n",
    "    return valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Batch Loss at Epoch 0 step 0: 15.6532\n",
      "Train Batch Loss at Epoch 0 step 20: 2.7414\n",
      "Train Batch Loss at Epoch 0 step 40: 2.1329\n",
      "Train loss at Epoch 0: 3.2627 Time taken 75.892 \n",
      "\t Val loss at Epoch 0: 3.6701 \n",
      "Train Batch Loss at Epoch 1 step 0: 3.6251\n",
      "Train Batch Loss at Epoch 1 step 20: 2.4028\n",
      "Train Batch Loss at Epoch 1 step 40: 2.3118\n",
      "Train loss at Epoch 1: 2.3610 Time taken 65.024 \n",
      "\t Val loss at Epoch 1: 2.9484 \n",
      "Train Batch Loss at Epoch 2 step 0: 2.9113\n",
      "Train Batch Loss at Epoch 2 step 20: 2.2216\n",
      "Train Batch Loss at Epoch 2 step 40: 2.2407\n",
      "Train loss at Epoch 2: 2.2414 Time taken 64.921 \n",
      "\t Val loss at Epoch 2: 2.0526 \n",
      "Train Batch Loss at Epoch 3 step 0: 2.0435\n",
      "Train Batch Loss at Epoch 3 step 20: 2.5713\n",
      "Train Batch Loss at Epoch 3 step 40: 2.1471\n",
      "Train loss at Epoch 3: 2.1793 Time taken 65.041 \n",
      "\t Val loss at Epoch 3: 2.0732 \n",
      "Train Batch Loss at Epoch 4 step 0: 2.0561\n",
      "Train Batch Loss at Epoch 4 step 20: 2.0376\n",
      "Train Batch Loss at Epoch 4 step 40: 2.0745\n",
      "Train loss at Epoch 4: 2.1290 Time taken 65.086 \n",
      "\t Val loss at Epoch 4: 2.0484 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-52f33a82fc5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mval_epoch_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#         print(grad)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    402\u001b[0m     \"\"\"\n\u001b[1;32m    403\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    593\u001b[0m           context.context().device_spec.device_type != \"CPU\"):\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/cpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    599\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0;31m# Store dataset reference to ensure that dataset is alive when this iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_apply_options\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    373\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         dataset = _OptimizeDataset(dataset, graph_rewrites,\n\u001b[0;32m--> 375\u001b[0;31m                                    graph_rewrite_configs)\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0;31m# (3) Apply autotune options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, optimizations, optimization_configs)\u001b[0m\n\u001b[1;32m   4363\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4364\u001b[0m         \u001b[0moptimization_configs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimization_configs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4365\u001b[0;31m         **self._flat_structure)\n\u001b[0m\u001b[1;32m   4366\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_OptimizeDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36moptimize_dataset\u001b[0;34m(input_dataset, optimizations, output_types, output_shapes, optimization_configs, name)\u001b[0m\n\u001b[1;32m   3677\u001b[0m         \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3678\u001b[0m         \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_shapes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"optimization_configs\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3679\u001b[0;31m         optimization_configs)\n\u001b[0m\u001b[1;32m   3680\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3681\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss=[]\n",
    "val_loss=[]\n",
    "for epoch in range(50):\n",
    "    t_start=time.time()\n",
    "    train_epoch_loss=tensorflow.keras.metrics.Mean()\n",
    "    val_epoch_loss=tensorflow.keras.metrics.Mean()\n",
    "    \n",
    "    for batch_idx,(x,y) in enumerate(train_dataset):\n",
    "        loss,grad=step(model,x,y)\n",
    "#         print(grad)\n",
    "        opt.apply_gradients(zip(grad,model.trainable_variables))\n",
    "        \n",
    "        \n",
    "        train_epoch_loss.update_state(loss)\n",
    "        if batch_idx%20==0:\n",
    "            print(\"Train Batch Loss at Epoch %d step %d: %.4f\"\n",
    "                % (epoch,batch_idx, float(loss)))\n",
    "    train_loss.append(train_epoch_loss.result())\n",
    "                  \n",
    "    for x_val,y_val in test_dataset:\n",
    "        v_loss=validate(x_val,y_val)\n",
    "        val_epoch_loss.update_state(v_loss)\n",
    "    print(\"Train loss at Epoch %d: %.4f Time taken %.3f \"%(epoch,train_epoch_loss.result(),time.time()-t_start))\n",
    "    print(\"\\t Val loss at Epoch %d: %.4f \"%(epoch,val_epoch_loss.result()))\n",
    "    val_loss.append(val_epoch_loss.result())\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../ssd/proxy_models/ae_epoch100_bceloss_ucmerced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(10):\n",
    "    index=np.random.randint(0,len(X_test_reshaped))\n",
    "\n",
    "    X_test_im=np.expand_dims(X_test_reshaped[index],0)\n",
    "    out_image=np.squeeze(model.predict(X_test_im))\n",
    "    \n",
    "    im_min=out_image.min(axis=(0, 1), keepdims=True)\n",
    "    im_max=out_image.max(axis=(0, 1), keepdims=True)\n",
    "    out_image=(out_image-im_min)/(im_max-im_min)\n",
    "    \n",
    "    \n",
    "    print(\"Orig \",np.min(X_test_im),np.max(X_test_im))\n",
    "    print(\"Gen \",np.min(out_image),np.max(out_image))\n",
    "    fig=plt.figure()\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(X_test_reshaped[index])\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(np.squeeze(X_test_im))\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(out_image)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SelfSup-(Autoencoder, UCMERCED).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
